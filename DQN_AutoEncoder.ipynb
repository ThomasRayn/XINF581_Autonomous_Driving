{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from AutoEncoder import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(image, height=83, width=83):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image.astype(float)\n",
    "    image /= 255.0\n",
    "    image = image[:height, :width]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dim_encoder(Hin, Win, padding, dilation, kernel_size, stride):\n",
    "    Hout = (Hin + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n",
    "    Wout = (Win + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n",
    "    return math.floor(Hout), math.floor(Wout)\n",
    "\n",
    "Hin, Win = 83, 83\n",
    "Hout, Wout = dim_encoder(Hin, Win, padding=(0, 0), dilation=(1, 1), kernel_size=(7, 7), stride=(3, 3))\n",
    "Hout, Wout = math.floor(Hout/2), math.floor(Wout/2)\n",
    "Hout, Wout = dim_encoder(Hout, Wout, padding=(0, 0), dilation=(1, 1), kernel_size=(4, 4), stride=(1, 1))\n",
    "Hout, Wout = math.floor(Hout/2), math.floor(Wout/2)\n",
    "\n",
    "print(Hout, Wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/thomas/ENSTA/X_IA/X-INF581/XINF581_Autonomous_Driving/DQN_AutoEncoder.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/thomas/ENSTA/X_IA/X-INF581/XINF581_Autonomous_Driving/DQN_AutoEncoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDQN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/thomas/ENSTA/X_IA/X-INF581/XINF581_Autonomous_Driving/DQN_AutoEncoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, action_space):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/thomas/ENSTA/X_IA/X-INF581/XINF581_Autonomous_Driving/DQN_AutoEncoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m(DQN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(20, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, len(action_space)),\n",
    "        )\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear_layers(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class DQNAgent:\n",
    "    def __init__(self, lr=0.001, weight_decay=1e-5, epochs=1, batch_size=128, memory_size=5000, epsilon=0.98, gamma=0.99):        \n",
    "        self.action_space = [\n",
    "            (0, 1, 0), (-1, 0, 0), \n",
    "            (1, 0, 0), (0, 0, 1),\n",
    "        ]\n",
    "        \n",
    "        # Memory for the experience replay\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        # Gamma for the Q update\n",
    "        self.gamma = gamma\n",
    "        # Epsilon to deal with the exploration/explotation trade-off\n",
    "        self.espilon = epsilon\n",
    "        \n",
    "        # Models of the agent\n",
    "        self.model = DQN(self.action_space).to(device)\n",
    "        self.target = DQN(self.action_space).to(device)\n",
    "        self.optim_model = optim.Adam(params=self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.optim_target = optim.Adam(params=self.target.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Variational AutoEncoder used by the agent\n",
    "        self.encoder = VAE()\n",
    "        self.encoder.load_state_dict(torch.load(\"models/vae\"))\n",
    "        \n",
    "    def play_action(self, state):\n",
    "        # Exploitation\n",
    "        if np.random.rand() < self.espilon:\n",
    "            state = state.to(device)\n",
    "            actions = self.model(state).cpu().detach().numpy()[0]\n",
    "            action = np.argmax(actions)\n",
    "        # Exploration\n",
    "        else:\n",
    "            action = np.random.randint(0, len(self.action_space))\n",
    "\n",
    "        return self.action_space[action]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = torch.from_numpy(X.astype('float32')).to(device)\n",
    "        y = torch.from_numpy(y.astype('float32'))\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = self.model(X).cpu()[:, 0, :]\n",
    "            loss = self.loss(y_pred, y)\n",
    "            self.optim_model.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim_model.step()\n",
    "            \n",
    "    def replay_memory(self):\n",
    "        # Random selection from memory to create a batch of data\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        train_state = []\n",
    "        train_target = []\n",
    "        \n",
    "        for state, action, reward, next_state, terminate in minibatch:\n",
    "            # Prediction of the target\n",
    "            state = state.to(device)\n",
    "            target = self.model(state).cpu().detach().numpy()[0]\n",
    "            if terminate:\n",
    "                target[action] = reward\n",
    "            # Update of the target according to the next state\n",
    "            else:\n",
    "                next_state = next_state.to(device)\n",
    "                t = self.target(next_state).cpu().detach().numpy()[0]\n",
    "                target[action] = reward + self.gamma * np.amax(t)\n",
    "            train_state.append(state.cpu().detach().numpy())\n",
    "            train_target.append(target)\n",
    "        # Gradient descent\n",
    "        self.fit(np.array(train_state), np.array(train_target))\n",
    "        \n",
    "    def encode_state(self, state):\n",
    "        # Encode the state from the sampling of the latent distribution\n",
    "        mu, log_var = self.encoder.encode(state)\n",
    "        return self.encoder.reparameterize(mu, log_var)\n",
    "            \n",
    "    def memorize(self, state, action, reward, next_state, terminate):\n",
    "        self.memory.append((state, self.action_space.index(action), reward, next_state, terminate))\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1179..1481 -> 302-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1265..1585 -> 320-tiles track\n",
      "Episode 1/100, total reward: -0.18244514106571832\n",
      "Track generation: 1056..1324 -> 268-tiles track\n",
      "Episode 2/100, total reward: -0.07340823970032528\n",
      "Track generation: 1182..1482 -> 300-tiles track\n",
      "Episode 3/100, total reward: -0.04414715719056625\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Episode 4/100, total reward: -0.10810810810806581\n",
      "Track generation: 1231..1552 -> 321-tiles track\n",
      "Episode 5/100, total reward: -0.1499999999999947\n",
      "Track generation: 1159..1453 -> 294-tiles track\n",
      "Episode 6/100, total reward: -0.16109215017063563\n",
      "Track generation: 1027..1290 -> 263-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1106..1394 -> 288-tiles track\n",
      "Episode 7/100, total reward: -0.01951219512219035\n",
      "Track generation: 1035..1298 -> 263-tiles track\n",
      "Episode 8/100, total reward: -0.16641221374044748\n",
      "Track generation: 1190..1491 -> 301-tiles track\n",
      "Episode 9/100, total reward: -0.19999999999998122\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Episode 10/100, total reward: -0.030508474576264016\n",
      "Track generation: 1061..1334 -> 273-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1205..1510 -> 305-tiles track\n",
      "Episode 11/100, total reward: -0.04210526315787197\n",
      "Track generation: 1291..1618 -> 327-tiles track\n",
      "Episode 12/100, total reward: -0.12024539877319074\n",
      "Track generation: 1068..1339 -> 271-tiles track\n",
      "Episode 13/100, total reward: -0.18518518518516883\n",
      "Track generation: 1084..1359 -> 275-tiles track\n",
      "Episode 14/100, total reward: -0.0029197080291105404\n",
      "Track generation: 1220..1529 -> 309-tiles track\n",
      "Episode 15/100, total reward: -0.059740259740251433\n",
      "Track generation: 1159..1458 -> 299-tiles track\n",
      "Episode 16/100, total reward: -0.01879194630933087\n",
      "Track generation: 1057..1325 -> 268-tiles track\n",
      "Episode 17/100, total reward: -0.16404494382021775\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Episode 18/100, total reward: -0.1959183673469204\n",
      "Track generation: 1242..1557 -> 315-tiles track\n",
      "Episode 19/100, total reward: -0.12229299363050006\n",
      "Track generation: 1249..1565 -> 316-tiles track\n",
      "Episode 20/100, total reward: -0.07619047619046726\n",
      "Track generation: 1249..1565 -> 316-tiles track\n",
      "Episode 21/100, total reward: -0.05396825396816429\n",
      "Track generation: 1300..1629 -> 329-tiles track\n",
      "Episode 22/100, total reward: -0.10243902439023922\n",
      "Track generation: 1054..1322 -> 268-tiles track\n",
      "Episode 23/100, total reward: -0.12958801498129235\n",
      "Track generation: 1389..1741 -> 352-tiles track\n",
      "Episode 24/100, total reward: -0.10199430199429893\n",
      "Track generation: 1189..1490 -> 301-tiles track\n",
      "Episode 25/100, total reward: -0.1333333333332899\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Episode 26/100, total reward: -0.18333333333331994\n",
      "Track generation: 1136..1429 -> 293-tiles track\n",
      "Episode 27/100, total reward: -0.07671232876707923\n",
      "Track generation: 1145..1435 -> 290-tiles track\n",
      "Episode 28/100, total reward: -0.15916955017300052\n",
      "Track generation: 990..1243 -> 253-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1151..1443 -> 292-tiles track\n",
      "Episode 29/100, total reward: -0.12714776632301705\n",
      "Track generation: 1228..1539 -> 311-tiles track\n",
      "Episode 30/100, total reward: -0.06451612903215787\n",
      "Track generation: 1291..1618 -> 327-tiles track\n",
      "Episode 31/100, total reward: -0.18773006134965056\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Episode 32/100, total reward: -0.062190812721067745\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "Episode 33/100, total reward: -0.03322259136211178\n",
      "Track generation: 973..1220 -> 247-tiles track\n",
      "Episode 34/100, total reward: -0.08455284552845055\n",
      "Track generation: 1157..1459 -> 302-tiles track\n",
      "Episode 35/100, total reward: -0.16611295681078855\n",
      "Track generation: 1251..1568 -> 317-tiles track\n",
      "Episode 36/100, total reward: -0.048101265822723976\n",
      "Track generation: 1056..1323 -> 267-tiles track\n",
      "Episode 37/100, total reward: -0.12781954887244462\n",
      "Track generation: 1373..1721 -> 348-tiles track\n",
      "Episode 38/100, total reward: -0.14524495677231075\n",
      "Track generation: 999..1258 -> 259-tiles track\n",
      "Episode 39/100, total reward: -0.0046511627913077\n",
      "Track generation: 1103..1383 -> 280-tiles track\n",
      "Episode 40/100, total reward: 1.7741935483867093\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "Episode 41/100, total reward: -0.03135888501741421\n",
      "Track generation: 1307..1638 -> 331-tiles track\n",
      "Episode 42/100, total reward: -0.024242424242442356\n",
      "Track generation: 1086..1365 -> 279-tiles track\n",
      "Episode 43/100, total reward: 23.953956834532036\n",
      "Track generation: 1204..1509 -> 305-tiles track\n",
      "Episode 44/100, total reward: -0.13157894736840314\n",
      "Track generation: 1050..1317 -> 267-tiles track\n",
      "Episode 45/100, total reward: -0.015037593985895742\n",
      "Track generation: 1116..1407 -> 291-tiles track\n",
      "Episode 46/100, total reward: -0.15862068965513001\n",
      "Track generation: 1269..1590 -> 321-tiles track\n",
      "Episode 47/100, total reward: 9.674999999999955\n",
      "Track generation: 957..1203 -> 246-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Episode 48/100, total reward: -0.1333333333336345\n",
      "Track generation: 1142..1440 -> 298-tiles track\n",
      "Episode 49/100, total reward: -0.19191919191991116\n",
      "Track generation: 1218..1524 -> 306-tiles track\n",
      "Episode 50/100, total reward: -0.14098360655727904\n",
      "Track generation: 1019..1282 -> 263-tiles track\n",
      "Episode 51/100, total reward: -0.06564885496176953\n",
      "Track generation: 1098..1377 -> 279-tiles track\n",
      "Episode 52/100, total reward: -0.07194244604394595\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Episode 53/100, total reward: -0.05594405594422125\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Episode 54/100, total reward: -0.05423728813545492\n",
      "Track generation: 1312..1644 -> 332-tiles track\n",
      "Episode 55/100, total reward: -0.13655589123866607\n",
      "Track generation: 1212..1519 -> 307-tiles track\n",
      "Episode 56/100, total reward: -0.11241830065362957\n",
      "Track generation: 1193..1495 -> 302-tiles track\n",
      "Episode 57/100, total reward: -0.19933554817301027\n",
      "Track generation: 1199..1509 -> 310-tiles track\n",
      "Episode 58/100, total reward: 5.323624595468605\n",
      "Track generation: 1465..1836 -> 371-tiles track\n",
      "Episode 59/100, total reward: -0.1729729729729105\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Episode 60/100, total reward: -0.17647058823561826\n",
      "Track generation: 1112..1394 -> 282-tiles track\n",
      "Episode 61/100, total reward: -0.025622775801310754\n",
      "Track generation: 1164..1459 -> 295-tiles track\n",
      "Episode 62/100, total reward: -0.175510204081769\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "Episode 63/100, total reward: -0.19047619047612555\n",
      "Track generation: 1105..1390 -> 285-tiles track\n",
      "Episode 64/100, total reward: -0.02535211267602408\n",
      "Track generation: 1239..1553 -> 314-tiles track\n",
      "Episode 65/100, total reward: -0.0562300319488902\n",
      "Track generation: 1066..1344 -> 278-tiles track\n",
      "Episode 66/100, total reward: -0.09891696750903844\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "Episode 67/100, total reward: -0.10860927152342692\n",
      "Track generation: 1278..1602 -> 324-tiles track\n",
      "Episode 68/100, total reward: -0.024148606811111167\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Episode 69/100, total reward: -0.06431095406350207\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Episode 70/100, total reward: -0.12616487455188585\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Episode 71/100, total reward: -0.05000000000022031\n",
      "Track generation: 1334..1671 -> 337-tiles track\n",
      "Episode 72/100, total reward: -0.19047619047616018\n",
      "Track generation: 1166..1471 -> 305-tiles track\n",
      "Episode 73/100, total reward: -0.01052631578948146\n",
      "Track generation: 966..1214 -> 248-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1205..1510 -> 305-tiles track\n",
      "Episode 74/100, total reward: -0.015789473684155925\n",
      "Track generation: 1191..1493 -> 302-tiles track\n",
      "Episode 75/100, total reward: -0.0996677740863181\n",
      "Track generation: 1064..1342 -> 278-tiles track\n",
      "Episode 76/100, total reward: -0.13935018050536707\n",
      "Track generation: 1161..1462 -> 301-tiles track\n",
      "Episode 77/100, total reward: -0.1333333333332899\n",
      "Track generation: 1060..1333 -> 273-tiles track\n",
      "Episode 78/100, total reward: -0.08235294117638614\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Episode 79/100, total reward: -0.19797979797974433\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "Episode 80/100, total reward: -0.07323943661965981\n",
      "Track generation: 1224..1534 -> 310-tiles track\n",
      "Episode 81/100, total reward: -0.0012944983817686218\n",
      "Track generation: 1173..1470 -> 297-tiles track\n",
      "Episode 82/100, total reward: -0.06486486486485554\n",
      "Track generation: 1018..1288 -> 270-tiles track\n",
      "Episode 83/100, total reward: -0.025278810408857683\n",
      "Track generation: 1129..1415 -> 286-tiles track\n",
      "Episode 84/100, total reward: -0.18596491228056392\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Episode 85/100, total reward: -0.19603960396027956\n",
      "Track generation: 916..1153 -> 237-tiles track\n",
      "Episode 86/100, total reward: -0.08813559322031428\n",
      "Track generation: 1204..1518 -> 314-tiles track\n",
      "Episode 87/100, total reward: -0.025559105431287338\n",
      "Track generation: 1127..1413 -> 286-tiles track\n",
      "Episode 88/100, total reward: -0.08070175438638683\n",
      "Track generation: 1142..1440 -> 298-tiles track\n",
      "Episode 89/100, total reward: -0.09898989898987876\n",
      "Track generation: 1173..1470 -> 297-tiles track\n",
      "Episode 90/100, total reward: -0.06486486486484933\n",
      "Track generation: 1280..1604 -> 324-tiles track\n",
      "Episode 91/100, total reward: -0.11207430340556945\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Episode 92/100, total reward: -0.10859106529202445\n",
      "Track generation: 1016..1278 -> 262-tiles track\n",
      "Episode 93/100, total reward: -0.0973180076629323\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "Episode 94/100, total reward: -0.06501766784448759\n",
      "Track generation: 1151..1443 -> 292-tiles track\n",
      "Episode 95/100, total reward: -0.017869415807546057\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Episode 96/100, total reward: -0.06486486486484577\n",
      "Track generation: 1132..1423 -> 291-tiles track\n",
      "Episode 97/100, total reward: -0.05517241379308335\n",
      "Track generation: 1095..1373 -> 278-tiles track\n",
      "Episode 98/100, total reward: -0.17978339350179665\n",
      "Track generation: 1191..1493 -> 302-tiles track\n",
      "Episode 99/100, total reward: -0.05514950166104815\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "Episode 100/100, total reward: -0.04210526315788529\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.envs.box2d.car_racing import CarRacing\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "EPISODES = 100\n",
    "BATCH_SIZE = 128\n",
    "UPDATE_TARGET_FREQUENCY = 5\n",
    "\n",
    "env = gym.make(\"CarRacing-v0\")\n",
    "car = DQNAgent()\n",
    "reward_history = []\n",
    "\n",
    "file = open(\"logs/dqn_ae/dqn_ae_log.txt\", \"w\")\n",
    "\n",
    "try:\n",
    "    for eps in range(1, EPISODES+1):        \n",
    "        \n",
    "        # Initialisation\n",
    "        init_state = env.reset()\n",
    "        init_state = image_processing(init_state)\n",
    "        init_state = np.expand_dims(init_state, axis=0)\n",
    "        init_state = torch.from_numpy(init_state.astype('float32')).unsqueeze(dim=0)\n",
    "        \n",
    "        total_reward = 0\n",
    "        counter_non_moving = 0\n",
    "        terminate = False\n",
    "        \n",
    "        # Encoding of the state\n",
    "        current_state = car.encode_state(init_state)\n",
    "        for _ in range(50):\n",
    "            env.step(None)\n",
    "        \n",
    "        while True:\n",
    "            # Predict action\n",
    "            action = car.play_action(current_state)\n",
    "            \n",
    "            # Play the action for 2 frames\n",
    "            reward = 0\n",
    "            for _ in range(2):\n",
    "                next_state, r, terminate, info = env.step(action)\n",
    "                reward += r\n",
    "                if terminate:\n",
    "                    break\n",
    "            \n",
    "            # Count the number of times the agent interrupts in a row \n",
    "            # Avoid long episode where the agent does not move at all\n",
    "            if action[1] != 1:\n",
    "                counter_non_moving += 1\n",
    "            else:\n",
    "                counter_non_moving = 0\n",
    "            \n",
    "            # Update of the reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Memorization\n",
    "            next_state = image_processing(next_state)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            next_state = torch.from_numpy(next_state.astype('float32')).unsqueeze(dim=0)\n",
    "            \n",
    "            # Encode the state\n",
    "            next_state = car.encode_state(next_state)\n",
    "            car.memorize(current_state, action, reward, next_state, terminate)\n",
    "            \n",
    "            # Termination conditions check\n",
    "            if terminate or total_reward < 0 or counter_non_moving >= 50:\n",
    "                file.write(f\"Episode {eps}/{EPISODES}, total reward: {total_reward}\\n\")\n",
    "                print(f\"Episode {eps}/{EPISODES}, total reward: {total_reward}\")\n",
    "                break\n",
    "            \n",
    "            # Experience replay\n",
    "            if len(car.memory) > BATCH_SIZE:\n",
    "                car.replay_memory()\n",
    "                \n",
    "            current_state = next_state\n",
    "                \n",
    "        # Update of the target model\n",
    "        if eps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            car.update_target()\n",
    "            \n",
    "        # Update reward history over time\n",
    "        reward_history.append(total_reward)\n",
    "            \n",
    "except (Exception, KeyboardInterrupt) as e:\n",
    "    logging.error(traceback.format_exc())\n",
    "        \n",
    "file.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
